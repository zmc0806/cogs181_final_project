{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LlY3XIqQcWTR"
   },
   "outputs": [],
   "source": [
    "# char_rnn_part1.py\n",
    "# Character-level RNN Language Model - Part 1: Data Processing\n",
    "# PyTorch implementation of Andrej Karpathy's char-rnn in Python\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "\n",
    "class TextDataset:\n",
    "    \"\"\"\n",
    "    Dataset class for loading and preprocessing text files\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir: str, seq_length: int, batch_size: int, split_fractions: List[float] = [0.9, 0.05, 0.05]):\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "\n",
    "        Args:\n",
    "            data_dir: Directory containing the input.txt file\n",
    "            seq_length: Length of each sequence\n",
    "            batch_size: Batch size\n",
    "            split_fractions: Split ratios for training, validation, and test sets\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Check if the split ratios are valid\n",
    "        assert sum(split_fractions) <= 1.0, \"The sum of split fractions must be less than or equal to 1.0\"\n",
    "        self.split_fractions = split_fractions\n",
    "\n",
    "        input_file = self.data_dir / 'input.txt'\n",
    "        vocab_file = self.data_dir / 'vocab.pt'\n",
    "        tensor_file = self.data_dir / 'data.pt'\n",
    "\n",
    "        # Ensure the data directory exists\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "\n",
    "        # Check if preprocessing is needed\n",
    "        run_prepro = False\n",
    "        if not (vocab_file.exists() and tensor_file.exists()):\n",
    "            print('vocab.pt and data.pt do not exist. Preprocessing...')\n",
    "            run_prepro = True\n",
    "        elif input_file.exists():\n",
    "            # Check if the input file is more recent than the vocabulary file\n",
    "            input_time = os.path.getmtime(input_file)\n",
    "            vocab_time = os.path.getmtime(vocab_file)\n",
    "            tensor_time = os.path.getmtime(tensor_file)\n",
    "            if input_time > vocab_time or input_time > tensor_time:\n",
    "                print('vocab.pt or data.pt is outdated. Reprocessing...')\n",
    "                run_prepro = True\n",
    "\n",
    "        if run_prepro and input_file.exists():\n",
    "            # Preprocess the input text file\n",
    "            print(f'One-time setup: Preprocessing input text file {input_file}...')\n",
    "            self.text_to_tensor(input_file, vocab_file, tensor_file)\n",
    "        elif not input_file.exists():\n",
    "            raise FileNotFoundError(f\"Input file {input_file} does not exist\")\n",
    "\n",
    "        print('Loading data files...')\n",
    "        self.data = torch.load(tensor_file)\n",
    "        self.vocab_mapping = torch.load(vocab_file)\n",
    "\n",
    "        # Reverse mapping (index -> character)\n",
    "        self.inv_vocab_mapping = {idx: char for char, idx in self.vocab_mapping.items()}\n",
    "\n",
    "        # Ensure data is divisible by batch_size * seq_length\n",
    "        data_len = len(self.data)\n",
    "        if data_len % (batch_size * seq_length) != 0:\n",
    "            print('Truncating data to ensure even batch/sequence division')\n",
    "            # Truncate data to make it evenly divisible by batch_size * seq_length\n",
    "            trim_len = (data_len // (batch_size * seq_length)) * batch_size * seq_length\n",
    "            self.data = self.data[:trim_len]\n",
    "\n",
    "        # Compute vocabulary size\n",
    "        self.vocab_size = len(self.vocab_mapping)\n",
    "        print(f'Vocabulary size: {self.vocab_size}')\n",
    "\n",
    "        # Create batches\n",
    "        self.prepare_batches()\n",
    "\n",
    "        # Initialize batch pointers\n",
    "        self.batch_idx = {i: 0 for i in range(3)}  # train:0, val:1, test:2\n",
    "\n",
    "    def text_to_tensor(self, in_textfile: Path, out_vocabfile: Path, out_tensorfile: Path) -> None:\n",
    "        \"\"\"\n",
    "        Convert a text file to a tensor and save the vocabulary\n",
    "\n",
    "        Args:\n",
    "            in_textfile: Path to the input text file\n",
    "            out_vocabfile: Path to the output vocabulary file\n",
    "            out_tensorfile: Path to the output tensor file\n",
    "        \"\"\"\n",
    "        with open(in_textfile, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Create vocabulary mapping (character -> index)\n",
    "        print('Creating vocabulary mapping...')\n",
    "        chars = sorted(list(set(text)))\n",
    "        vocab_mapping = {char: i for i, char in enumerate(chars)}\n",
    "\n",
    "        # Convert text to tensor\n",
    "        print('Converting data to tensor...')\n",
    "        data = torch.LongTensor([vocab_mapping[char] for char in text])\n",
    "\n",
    "        # Save preprocessed files\n",
    "        print(f'Saving {out_vocabfile}')\n",
    "        torch.save(vocab_mapping, out_vocabfile)\n",
    "        print(f'Saving {out_tensorfile}')\n",
    "        torch.save(data, out_tensorfile)\n",
    "\n",
    "    def prepare_batches(self) -> None:\n",
    "        \"\"\"\n",
    "        Prepare batches for training\n",
    "        \"\"\"\n",
    "        # Reshape data to (batch_size, n)\n",
    "        data_len = len(self.data)\n",
    "        self.num_batches = data_len // (self.batch_size * self.seq_length)\n",
    "\n",
    "        # Reshape to (batch_size, -1)\n",
    "        xdata = self.data.view(self.batch_size, -1)\n",
    "        # Create y data, a shifted copy of x\n",
    "        ydata = xdata.clone()\n",
    "        ydata[:, :-1] = xdata[:, 1:]\n",
    "        ydata[:, -1] = xdata[:, 0]\n",
    "\n",
    "        # Split data into chunks of seq_length\n",
    "        self.x_batches = []\n",
    "        self.y_batches = []\n",
    "        for i in range(0, xdata.size(1), self.seq_length):\n",
    "            if i + self.seq_length <= xdata.size(1):\n",
    "                self.x_batches.append(xdata[:, i:i + self.seq_length])\n",
    "                self.y_batches.append(ydata[:, i:i + self.seq_length])\n",
    "\n",
    "        # Ensure the number of x and y batches match\n",
    "        assert len(self.x_batches) == len(self.y_batches)\n",
    "\n",
    "        # Compute training/validation/test split sizes\n",
    "        self.num_train = int(len(self.x_batches) * self.split_fractions[0])\n",
    "        self.num_val = int(len(self.x_batches) * self.split_fractions[1])\n",
    "        self.num_test = len(self.x_batches) - self.num_train - self.num_val\n",
    "\n",
    "        self.split_sizes = [self.num_train, self.num_val, self.num_test]\n",
    "\n",
    "        print(f'Data loading complete. Training batches: {self.num_train}, Validation: {self.num_val}, Test: {self.num_test}')\n",
    "\n",
    "    def reset_batch_pointer(self, split_index: int, batch_index: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Reset batch pointer for a given split\n",
    "\n",
    "        Args:\n",
    "            split_index: Split index (0: training, 1: validation, 2: test)\n",
    "            batch_index: Batch index, default is 0\n",
    "        \"\"\"\n",
    "        self.batch_idx[split_index] = batch_index\n",
    "\n",
    "    def next_batch(self, split_index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get the next batch\n",
    "\n",
    "        Args:\n",
    "            split_index: Split index (0: training, 1: validation, 2: test)\n",
    "\n",
    "        Returns:\n",
    "            (x_batch, y_batch): Input and target batches\n",
    "        \"\"\"\n",
    "        if self.split_sizes[split_index] == 0:\n",
    "            split_names = ['train', 'val', 'test']\n",
    "            raise ValueError(f'Error: Requested batch for split {split_names[split_index]}, but this split has no data.')\n",
    "\n",
    "        # Update batch pointer\n",
    "        idx = self.batch_idx[split_index]\n",
    "        self.batch_idx[split_index] = (idx + 1) % self.split_sizes[split_index]\n",
    "\n",
    "        # Compute actual index\n",
    "        if split_index == 1:  # Validation set\n",
    "            idx += self.num_train\n",
    "        elif split_index == 2:  # Test set\n",
    "            idx += self.num_train + self.num_val\n",
    "\n",
    "        return self.x_batches[idx], self.y_batches[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GL_3P3arcXN9"
   },
   "outputs": [],
   "source": [
    "# char_rnn_part2.py\n",
    "# Character-level RNN Language Model - Part 2: Model Definition\n",
    "# PyTorch implementation of Andrej Karpathy's char-rnn in Python\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Character-level RNN Language Model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, input_size: int, hidden_size: int, num_layers: int,\n",
    "                model_type: str = 'lstm', dropout: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize the CharRNN model\n",
    "\n",
    "        Args:\n",
    "            vocab_size: Vocabulary size\n",
    "            input_size: Input size for one-hot encoding (usually equal to vocab_size)\n",
    "            hidden_size: Hidden layer size\n",
    "            num_layers: Number of RNN layers\n",
    "            model_type: Type of RNN ('rnn', 'lstm', or 'gru')\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.model_type = model_type.lower()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Embedding layer\n",
    "        self.encoder = nn.Embedding(vocab_size, input_size)\n",
    "\n",
    "        # Select RNN type\n",
    "        if self.model_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        elif self.model_type == 'gru':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        elif self.model_type == 'rnn':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, nonlinearity='tanh', dropout=dropout, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown RNN type: {self.model_type}. Must be 'rnn', 'lstm', or 'gru'\")\n",
    "\n",
    "        # Decoder (converts to output size)\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights\n",
    "        \"\"\"\n",
    "        # Initialize embedding weights\n",
    "        init_range = 0.1\n",
    "        self.encoder.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "        # Initialize decoder weights\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "        # If LSTM, initialize forget gate bias to 1\n",
    "        if self.model_type == 'lstm':\n",
    "            for names in self.rnn._all_weights:\n",
    "                for name in filter(lambda n: \"bias\" in n, names):\n",
    "                    bias = getattr(self.rnn, name)\n",
    "                    n = bias.size(0)\n",
    "                    start, end = n // 4, n // 2\n",
    "                    bias.data[start:end].fill_(1.0)\n",
    "\n",
    "    def init_hidden(self, batch_size: int, device: torch.device) -> Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Initialize hidden state\n",
    "\n",
    "        Args:\n",
    "            batch_size: Batch size\n",
    "            device: Computing device\n",
    "\n",
    "        Returns:\n",
    "            Initialized hidden state\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters())\n",
    "        if self.model_type == 'lstm':\n",
    "            return (weight.new_zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "                    weight.new_zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
    "        else:\n",
    "            return weight.new_zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, hidden: Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]) -> Tuple[torch.Tensor, Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "\n",
    "        Args:\n",
    "            input: Input sequence [batch_size, seq_length]\n",
    "            hidden: Initial hidden state\n",
    "\n",
    "        Returns:\n",
    "            output: Output sequence [batch_size, seq_length, vocab_size]\n",
    "            hidden: Final hidden state\n",
    "        \"\"\"\n",
    "        # Embed input\n",
    "        emb = self.encoder(input)  # [batch_size, seq_length, input_size]\n",
    "\n",
    "        # Run RNN\n",
    "        output, hidden = self.rnn(emb, hidden)  # [batch_size, seq_length, hidden_size], hidden_state\n",
    "\n",
    "        # Decode output\n",
    "        decoded = self.decoder(output)  # [batch_size, seq_length, vocab_size]\n",
    "\n",
    "        return decoded, hidden\n",
    "\n",
    "    def forward_step(self, input: torch.Tensor, hidden: Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]) -> Tuple[torch.Tensor, Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Single-step forward propagation for text generation\n",
    "\n",
    "        Args:\n",
    "            input: Single input [batch_size, 1]\n",
    "            hidden: Hidden state\n",
    "\n",
    "        Returns:\n",
    "            output: Output [batch_size, vocab_size]\n",
    "            hidden: Updated hidden state\n",
    "        \"\"\"\n",
    "        # Embed input\n",
    "        emb = self.encoder(input)  # [batch_size, 1, input_size]\n",
    "\n",
    "        # Run RNN\n",
    "        output, hidden = self.rnn(emb, hidden)  # [batch_size, 1, hidden_size], hidden_state\n",
    "\n",
    "        # Decode output\n",
    "        decoded = self.decoder(output.squeeze(1))  # [batch_size, vocab_size]\n",
    "\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xub-XAMHcZLQ"
   },
   "outputs": [],
   "source": [
    "# char_rnn_part3.py\n",
    "# Character-level RNN Language Model - Part 3: Training and Evaluation Functions\n",
    "# PyTorch implementation of Andrej Karpathy's char-rnn in Python\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Ensure necessary classes are imported from Part 1 and Part 2\n",
    "# from char_rnn_part1 import TextDataset\n",
    "# from char_rnn_part2 import CharRNN\n",
    "\n",
    "def train(model: nn.Module, data_loader, optimizer: torch.optim.Optimizer,\n",
    "         criterion: nn.Module, device: torch.device,\n",
    "         grad_clip: float = 5.0, print_every: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "\n",
    "    Args:\n",
    "        model: The model\n",
    "        data_loader: Data loader\n",
    "        optimizer: Optimizer\n",
    "        criterion: Loss function\n",
    "        device: Computing device\n",
    "        grad_clip: Gradient clipping value\n",
    "        print_every: Print progress every N batches\n",
    "\n",
    "    Returns:\n",
    "        Average loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    data_loader.reset_batch_pointer(0)  # Reset training pointer\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(data_loader.batch_size, device)\n",
    "\n",
    "    for i in range(data_loader.num_train):\n",
    "        # Get batch\n",
    "        x, y = data_loader.next_batch(0)  # Get training batch\n",
    "\n",
    "        # Move to device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Detach hidden state\n",
    "        if isinstance(hidden, tuple):\n",
    "            hidden = tuple(h.detach() for h in hidden)\n",
    "        else:\n",
    "            hidden = hidden.detach()\n",
    "\n",
    "        # Forward pass\n",
    "        output, hidden = model(x, hidden)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output.view(-1, data_loader.vocab_size), y.view(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print progress\n",
    "        if (i + 1) % print_every == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'| Batch {i + 1}/{data_loader.num_train} | '\n",
    "                  f'Time {elapsed:.2f}s | Loss {loss.item():.4f}')\n",
    "            start_time = time.time()\n",
    "\n",
    "    return total_loss / data_loader.num_train\n",
    "\n",
    "def evaluate(model: nn.Module, data_loader, criterion: nn.Module,\n",
    "           device: torch.device, split_index: int) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "\n",
    "    Args:\n",
    "        model: The model\n",
    "        data_loader: Data loader\n",
    "        criterion: Loss function\n",
    "        device: Computing device\n",
    "        split_index: Split index (1: validation, 2: test)\n",
    "\n",
    "    Returns:\n",
    "        Average loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data_loader.reset_batch_pointer(split_index)  # Reset validation/test pointer\n",
    "    total_loss = 0\n",
    "    num_batches = data_loader.split_sizes[split_index]\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(data_loader.batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            # Get batch\n",
    "            x, y = data_loader.next_batch(split_index)\n",
    "\n",
    "            # Move to device\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output, hidden = model(x, hidden)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output.view(-1, data_loader.vocab_size), y.view(-1))\n",
    "\n",
    "            # Record loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def generate(model: nn.Module, data_loader, prime_text: str,\n",
    "             length: int, temperature: float, device: torch.device, topk: int = 0) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using the model\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        data_loader: Data loader (for vocabulary mapping)\n",
    "        prime_text: Seed text\n",
    "        length: Length of the generated text\n",
    "        temperature: Sampling temperature\n",
    "        device: Computing device\n",
    "        topk: Use top-k sampling (0 to disable)\n",
    "\n",
    "    Returns:\n",
    "        Generated text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    vocab_mapping = data_loader.vocab_mapping\n",
    "    inv_vocab_mapping = data_loader.inv_vocab_mapping\n",
    "\n",
    "    # Convert prime_text to index list\n",
    "    prime_idxs = [vocab_mapping.get(char, vocab_mapping.get(' ')) for char in prime_text]\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(1, device)\n",
    "\n",
    "    # Warm up the model\n",
    "    with torch.no_grad():\n",
    "        for p in prime_idxs[:-1]:\n",
    "            input_tensor = torch.tensor([[p]], dtype=torch.long).to(device)\n",
    "            _, hidden = model.forward_step(input_tensor, hidden)\n",
    "\n",
    "        # Last character\n",
    "        prev_char = torch.tensor([[prime_idxs[-1]]], dtype=torch.long).to(device)\n",
    "\n",
    "        # Generated result text, starting with prime_text\n",
    "        result_text = prime_text\n",
    "\n",
    "        # Start generation\n",
    "        for i in range(length):\n",
    "            # Single forward step\n",
    "            output, hidden = model.forward_step(prev_char, hidden)\n",
    "\n",
    "            # Apply temperature\n",
    "            output = output.div(temperature)\n",
    "            output = torch.exp(output)\n",
    "\n",
    "            # Optional: top-k sampling\n",
    "            if topk > 0:\n",
    "                topk = min(topk, output.size(-1))\n",
    "                top_v, top_idx = torch.topk(output, topk)\n",
    "                probs = top_v / top_v.sum()\n",
    "                # Sample from top-k\n",
    "                idx = torch.multinomial(probs, 1)\n",
    "                prev_char = top_idx[:, idx.item()].view(1, 1)\n",
    "            else:\n",
    "                # Standard sampling\n",
    "                probs = output / output.sum()\n",
    "                prev_char = torch.multinomial(probs, 1)\n",
    "\n",
    "            # Convert to character and add to result\n",
    "            char = inv_vocab_mapping[prev_char.item()]\n",
    "            result_text += char\n",
    "\n",
    "    return result_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "yHnLMrQ1cabZ",
    "outputId": "be6b393f-c448-438d-f6e6-09d0abe9d6ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--mode {train,sample}]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/zmc/Library/Jupyter/runtime/kernel-c178ecc0-8e51-4d2b-a771-a78442e7ed4a.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zmc/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# char_rnn_part4.py\n",
    "# Character-level RNN Language Model - Part 4: Main Training Loop and Utility Tools\n",
    "# PyTorch implementation of Andrej Karpathy's char-rnn in Python\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "\n",
    "# Ensure necessary functions are imported from previous parts\n",
    "# from char_rnn_part1 import TextDataset\n",
    "# from char_rnn_part2 import CharRNN\n",
    "# from char_rnn_part3 import train, evaluate, generate\n",
    "\n",
    "def train_model(data_dir: str, model_type: str = 'lstm', hidden_size: int = 128,\n",
    "               num_layers: int = 2, dropout: float = 0.0, seq_length: int = 50,\n",
    "               batch_size: int = 50, max_epochs: int = 50, learning_rate: float = 0.002,\n",
    "               lr_decay: float = 0.97, lr_decay_after: int = 10, grad_clip: float = 5.0,\n",
    "               checkpoint_dir: str = 'checkpoints'):\n",
    "    \"\"\"\n",
    "    Train a character-level RNN language model\n",
    "\n",
    "    Args:\n",
    "        data_dir: Directory containing input.txt file\n",
    "        model_type: Type of RNN ('rnn', 'lstm', or 'gru')\n",
    "        hidden_size: Size of the hidden layer\n",
    "        num_layers: Number of RNN layers\n",
    "        dropout: Dropout probability\n",
    "        seq_length: Sequence length\n",
    "        batch_size: Batch size\n",
    "        max_epochs: Maximum number of epochs\n",
    "        learning_rate: Learning rate\n",
    "        lr_decay: Learning rate decay factor\n",
    "        lr_decay_after: Number of epochs before learning rate starts decaying\n",
    "        grad_clip: Gradient clipping value\n",
    "        checkpoint_dir: Directory for saving checkpoints\n",
    "    \"\"\"\n",
    "    # Create the save directory\n",
    "    checkpoint_path = Path(checkpoint_dir)\n",
    "    checkpoint_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Check CUDA availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # Load dataset\n",
    "    data_loader = TextDataset(data_dir, seq_length, batch_size)\n",
    "\n",
    "    # Create model\n",
    "    vocab_size = data_loader.vocab_size\n",
    "    embedding_size = hidden_size  # Embedding size equals hidden size\n",
    "    model = CharRNN(vocab_size, embedding_size, hidden_size, num_layers, model_type, dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    # Create optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Print number of model parameters\n",
    "    params = sum([p.numel() for p in model.parameters()])\n",
    "    print(f'Total model parameters: {params}')\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=lr_decay)\n",
    "\n",
    "    # Save training and validation loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{max_epochs}')\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Training\n",
    "        train_loss = train(model, data_loader, optimizer, criterion, device, grad_clip)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        val_loss = evaluate(model, data_loader, criterion, device, 1)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'Training Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Learning rate decay\n",
    "        if epoch >= lr_decay_after:\n",
    "            scheduler.step()\n",
    "            print(f'Learning rate decayed to: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            checkpoint = {\n",
    "                'model_state': model.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "                'vocab_mapping': data_loader.vocab_mapping,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'params': {\n",
    "                    'model_type': model_type,\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'num_layers': num_layers,\n",
    "                    'vocab_size': vocab_size,\n",
    "                    'embedding_size': embedding_size,\n",
    "                    'dropout': dropout\n",
    "                }\n",
    "            }\n",
    "\n",
    "            model_filename = f'{model_type}_epoch{epoch+1:02d}_val{val_loss:.4f}.pt'\n",
    "            save_path = checkpoint_path / model_filename\n",
    "            torch.save(checkpoint, save_path)\n",
    "            print(f'Saved model to {save_path}')\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Plot training and validation loss curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(checkpoint_path / 'loss_curve.png')\n",
    "    plt.show()\n",
    "\n",
    "    return model, data_loader\n",
    "\n",
    "def load_model(checkpoint_path: str) -> Tuple[nn.Module, Dict]:\n",
    "    \"\"\"\n",
    "    Load a trained model\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path: Path to the checkpoint file\n",
    "\n",
    "    Returns:\n",
    "        model: Loaded model\n",
    "        vocab_mapping: Vocabulary mapping\n",
    "    \"\"\"\n",
    "    # Check if GPU is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Retrieve model parameters\n",
    "    params = checkpoint['params']\n",
    "    model_type = params['model_type']\n",
    "    hidden_size = params['hidden_size']\n",
    "    num_layers = params['num_layers']\n",
    "    vocab_size = params['vocab_size']\n",
    "    embedding_size = params['embedding_size']\n",
    "    dropout = params['dropout']\n",
    "\n",
    "    # Create model\n",
    "    model = CharRNN(vocab_size, embedding_size, hidden_size, num_layers, model_type, dropout)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Retrieve vocabulary mapping\n",
    "    vocab_mapping = checkpoint['vocab_mapping']\n",
    "\n",
    "    return model, vocab_mapping\n",
    "\n",
    "def sample_text(model_path: str, prime_text: str = '', length: int = 1000,\n",
    "               temperature: float = 1.0, data_dir: str = None, topk: int = 0):\n",
    "    \"\"\"\n",
    "    Generate text from a trained model\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to the model file\n",
    "        prime_text: Seed text\n",
    "        length: Length of generated text\n",
    "        temperature: Sampling temperature\n",
    "        data_dir: Data directory (if model is not preloaded)\n",
    "        topk: Use top-k sampling (0 to disable)\n",
    "\n",
    "    Returns:\n",
    "        Generated text\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load model\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "    # Retrieve model parameters\n",
    "    params = checkpoint['params']\n",
    "    model_type = params['model_type']\n",
    "    hidden_size = params['hidden_size']\n",
    "    num_layers = params['num_layers']\n",
    "    vocab_size = params['vocab_size']\n",
    "    embedding_size = params['embedding_size']\n",
    "    dropout = params['dropout']\n",
    "\n",
    "    # Create model\n",
    "    model = CharRNN(vocab_size, embedding_size, hidden_size, num_layers, model_type, dropout)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Retrieve vocabulary mapping\n",
    "    vocab_mapping = checkpoint['vocab_mapping']\n",
    "\n",
    "    # Create reverse mapping (index -> character)\n",
    "    inv_vocab_mapping = {idx: char for char, idx in vocab_mapping.items()}\n",
    "\n",
    "    # If data_dir is not provided, create a temporary data loader for sampling\n",
    "    class TempDataLoader:\n",
    "        def __init__(self, vocab_mapping, inv_vocab_mapping):\n",
    "            self.vocab_mapping = vocab_mapping\n",
    "            self.inv_vocab_mapping = inv_vocab_mapping\n",
    "            self.vocab_size = len(vocab_mapping)\n",
    "\n",
    "    data_loader = TempDataLoader(vocab_mapping, inv_vocab_mapping)\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = generate(model, data_loader, prime_text, length, temperature, device, topk)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Character-level RNN Language Model')\n",
    "\n",
    "    # Define mode\n",
    "    parser.add_argument('--mode', type=str, default='train', choices=['train', 'sample'],\n",
    "                      help='Operation mode: train or sample')\n",
    "\n",
    "    # Training parameters...\n",
    "    # (Arguments remain unchanged)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.mode == 'train':\n",
    "        train_model(**vars(args))\n",
    "    elif args.mode == 'sample':\n",
    "        text = sample_text(**vars(args))\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Generated Text (Temperature={args.temperature}):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(text)\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MkLMOjvrczap"
   },
   "outputs": [],
   "source": [
    "# Define a training function that allows you to specify a custom name for saving the model\n",
    "def train_with_config(data_dir, model_name, custom_save_name=None, model_type='lstm',\n",
    "                     hidden_size=256, num_layers=2, dropout=0.3, seq_length=100, batch_size=32):\n",
    "    # Use the custom name if provided, otherwise use the model_name\n",
    "    save_name = custom_save_name if custom_save_name else model_name\n",
    "    checkpoint_dir = f'checkpoints_{save_name}'\n",
    "\n",
    "    model, data_loader = train_model(\n",
    "        data_dir=data_dir,\n",
    "        model_type=model_type,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        seq_length=seq_length,\n",
    "        batch_size=batch_size,\n",
    "        max_epochs=30,\n",
    "        learning_rate=0.002,\n",
    "        lr_decay=0.97,\n",
    "        lr_decay_after=10,\n",
    "        grad_clip=5.0,\n",
    "        checkpoint_dir=checkpoint_dir\n",
    "    )\n",
    "\n",
    "    # After training, you can rename the checkpoint file if needed\n",
    "    import glob\n",
    "    import os\n",
    "    checkpoint_files = glob.glob(f'{checkpoint_dir}/*.pt')\n",
    "    if checkpoint_files:\n",
    "        best_checkpoint = min(checkpoint_files,\n",
    "                             key=lambda x: float(x.split('val')[-1].split('.pt')[0]))\n",
    "\n",
    "        # Create a new filename with your custom name\n",
    "        new_filename = os.path.join(checkpoint_dir,\n",
    "                                   f\"{save_name}_final.pt\")\n",
    "\n",
    "        # Make a copy of the best checkpoint with the new name\n",
    "        import shutil\n",
    "        shutil.copy(best_checkpoint, new_filename)\n",
    "        print(f\"Best model saved as: {new_filename}\")\n",
    "\n",
    "    return model, data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "09VNdFKc3xSS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "vocab.pt and data.pt do not exist. Preprocessing...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Input file data/scifi/input.txt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now train your models with custom names\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m scifi_model, scifi_loader \u001b[38;5;241m=\u001b[39m train_with_config(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/scifi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscifi\u001b[39m\u001b[38;5;124m'\u001b[39m, custom_save_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSciFi_Adventure_Model\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m384\u001b[39m, seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m mixed_model, mixed_loader \u001b[38;5;241m=\u001b[39m train_with_config(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/mixed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m'\u001b[39m, custom_save_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSciFi_Romance_Hybrid\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgru\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m, in \u001b[0;36mtrain_with_config\u001b[0;34m(data_dir, model_name, custom_save_name, model_type, hidden_size, num_layers, dropout, seq_length, batch_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m save_name \u001b[38;5;241m=\u001b[39m custom_save_name \u001b[38;5;28;01mif\u001b[39;00m custom_save_name \u001b[38;5;28;01melse\u001b[39;00m model_name\n\u001b[1;32m      6\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m model, data_loader \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m      9\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m     10\u001b[0m     model_type\u001b[38;5;241m=\u001b[39mmodel_type,\n\u001b[1;32m     11\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     12\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39mnum_layers,\n\u001b[1;32m     13\u001b[0m     dropout\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[1;32m     14\u001b[0m     seq_length\u001b[38;5;241m=\u001b[39mseq_length,\n\u001b[1;32m     15\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     16\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m     17\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.002\u001b[39m,\n\u001b[1;32m     18\u001b[0m     lr_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.97\u001b[39m,\n\u001b[1;32m     19\u001b[0m     lr_decay_after\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     20\u001b[0m     grad_clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m,\n\u001b[1;32m     21\u001b[0m     checkpoint_dir\u001b[38;5;241m=\u001b[39mcheckpoint_dir\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# After training, you can rename the checkpoint file if needed\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 55\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(data_dir, model_type, hidden_size, num_layers, dropout, seq_length, batch_size, max_epochs, learning_rate, lr_decay, lr_decay_after, grad_clip, checkpoint_dir)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m TextDataset(data_dir, seq_length, batch_size)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[1;32m     58\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m data_loader\u001b[38;5;241m.\u001b[39mvocab_size\n",
      "Cell \u001b[0;32mIn[7], line 65\u001b[0m, in \u001b[0;36mTextDataset.__init__\u001b[0;34m(self, data_dir, seq_length, batch_size, split_fractions)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_to_tensor(input_file, vocab_file, tensor_file)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m input_file\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading data files...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(tensor_file)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Input file data/scifi/input.txt does not exist"
     ]
    }
   ],
   "source": [
    "# Now train your models with custom names\n",
    "scifi_model, scifi_loader = train_with_config(\n",
    "    'data/scifi', 'scifi', custom_save_name='SciFi_Adventure_Model',\n",
    "    hidden_size=384, seq_length=150, model_type='lstm')\n",
    "\n",
    "\n",
    "mixed_model, mixed_loader = train_with_config(\n",
    "    'data/mixed', 'mixed', custom_save_name='SciFi_Romance_Hybrid',\n",
    "    hidden_size=512, num_layers=3, dropout=0.4, model_type='gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YhAaf-4TnA5z",
    "outputId": "c876b2b0-85ac-4441-a010-ce28a5330c02"
   },
   "outputs": [],
   "source": [
    "romance_model, romance_loader = train_with_config(\n",
    "    'data/romance', 'romance', custom_save_name='Classic_Romance_Model',\n",
    "    hidden_size=256, dropout=0.2, model_type='lstm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EUJVTVYNsiz3",
    "outputId": "d3e6df15-44ac-4369-e7d5-139300503ca4"
   },
   "outputs": [],
   "source": [
    "mixed_model, mixed_loader = train_with_config(\n",
    "    'data/mixed', 'mixed', custom_save_name='SciFi_Romance_Hybrid',\n",
    "    hidden_size=512, num_layers=3, dropout=0.4, model_type='gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "LbghIjPhyE3c",
    "outputId": "6df79ca8-f8c8-401e-9eb7-9c775a061ce2"
   },
   "outputs": [],
   "source": [
    "mixed_model, mixed_loader = train_with_config(\n",
    "    'data/mixed', 'mixed', custom_save_name='SciFi_Romance_Hybrid',\n",
    "    hidden_size=512, num_layers=3, dropout=0.4, model_type='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAeE7QZR39mB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model_name, prompts, temperature=0.8, topk=5, length=500):\n",
    "    \"\"\"Generate multiple text samples from a trained model\"\"\"\n",
    "    import glob\n",
    "    import os\n",
    "    \n",
    "    # Find the best checkpoint (lowest validation loss)\n",
    "    checkpoint_files = glob.glob(f'checkpoints_{model_name}/*.pt')\n",
    "    if not checkpoint_files:\n",
    "        print(f\"No checkpoint files found for {model_name}\")\n",
    "        return\n",
    "    \n",
    "    # Get the checkpoint with the lowest validation loss\n",
    "    best_checkpoint = min(checkpoint_files, \n",
    "                         key=lambda x: float(x.split('val')[-1].split('.pt')[0]) \n",
    "                         if 'val' in x else float('inf'))\n",
    "    \n",
    "    print(f\"\\n===== GENERATING TEXT FROM {model_name.upper()} MODEL =====\")\n",
    "    print(f\"Using checkpoint: {os.path.basename(best_checkpoint)}\")\n",
    "    \n",
    "    results = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"\\nPrompt {i+1}: \\\"{prompt}\\\"\")\n",
    "        generated = sample_text(\n",
    "            model_path=best_checkpoint,\n",
    "            prime_text=prompt,\n",
    "            length=length,\n",
    "            temperature=temperature,\n",
    "            topk=topk\n",
    "        )\n",
    "        print(f\"Generated text:\\n{'-'*60}\\n{generated}\\n{'-'*60}\")\n",
    "        results.append(generated)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test prompts for different genres\n",
    "sci_fi_prompts = [\n",
    "    \"The starship's engines hummed as it approached the nebula, \",\n",
    "    \"In the year 2150, the first quantum AI achieved consciousness and \",\n",
    "    \"The alien artifact glowed with an eerie blue light when \"\n",
    "]\n",
    "\n",
    "romance_prompts = [\n",
    "    \"Her heart raced as she opened the letter, never expecting \",\n",
    "    \"Under the moonlit sky, their eyes met across the garden and \",\n",
    "    \"The duchess had sworn never to love again, but \"\n",
    "]\n",
    "\n",
    "mixed_prompts = [\n",
    "    \"The space station's observation deck was where she first saw him, \",\n",
    "    \"The android felt something strange in its circuits whenever she smiled, \",\n",
    "    \"Time travel was forbidden, but he couldn't bear to live without her, \"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== GENERATING TEXT FROM SCIFI_ADVENTURE_MODEL MODEL =====\n",
      "Using checkpoint: SciFi_Adventure_Model_final.pt\n",
      "\n",
      "Prompt 1: \"The starship's engines hummed as it approached the nebula, \"\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "The starship's engines hummed as it approached the nebula, an\n",
      "explains of the shells that the same striking that they were\n",
      "a shells, and a conveys are at last and waiting strange as a machine was a species\n",
      "of the horizon. The sharks and darkness, the hands of\n",
      "the part of the horizontal pole that his hand of several and time in the rich private\n",
      "sharks that seeming its shaped seasons on the _Nautilus_.\n",
      "\n",
      "At that moment this strange was he did not retrace the midst of\n",
      "the seas. This is a convinced of the horse was\n",
      "realised a mountain, but they would be desc\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt 2: \"In the year 2150, the first quantum AI achieved consciousness and \"\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "In the year 2150, the first quantum AI achieved consciousness and thought\n",
      "of the machine was thrown in the street.\n",
      "\n",
      "I could hear my eyes with his persuasion. It was the mouth of\n",
      "the pit. At the third of the sea, which was above the common\n",
      "which the machine that I was still, instead of a sheet of\n",
      "the pit of the marking. The morning was a crystal which was absolutely\n",
      "considerable to stop that the candles.\n",
      "\n",
      "I came to the carprate came upon the surface of the saloon. The minute\n",
      "was to be so seen in the carrier of them some sensible pressures with\n",
      "its pressure, and\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt 3: \"The alien artifact glowed with an eerie blue light when \"\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "The alien artifact glowed with an eerie blue light when I\n",
      "could not see if a stone of the stream. Here a strange head surrived\n",
      "it with several of the water, and the work of man, seemed a little blackness was\n",
      "succeeded with every and five or a stone, the waves of\n",
      "the seas and disappeared but a matter, and his part of the heaps of the\n",
      "window of the sea when we went up in sight of the surface of the\n",
      "saloon of the sea, the strong, and the sea was entirely became painful\n",
      "as that it was the large breadth. I had set up and dressed at\n",
      "the paper.\n",
      "\n",
      "The sharks \n",
      "------------------------------------------------------------\n",
      "\n",
      "===== GENERATING TEXT FROM CLASSIC_ROMANCE_MODEL MODEL =====\n",
      "Using checkpoint: Classic_Romance_Model_final.pt\n",
      "\n",
      "Prompt 1: \"Her heart raced as she opened the letter, never expecting \"\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "Her heart raced as she opened the letter, never expecting the matter was an embracity, and then it was a desire mistake and desired a moment. I had a little stronger at the stranger.\n",
      "\n",
      "“I do not be sure.”\n",
      "\n",
      "“When you are any occasion of your sister’s design.”\n",
      "\n",
      "“What!” I said says to him.\n",
      "\n",
      "“And thithertre will should not seem to say it in\n",
      "the subject of my lips of interval of the same particular table. I am sure you are indeed, and what is a strange and silence in a cold than I am sure it was a sound,\n",
      "sir, and so well as if he had seen it a long side as t\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt 2: \"Under the moonlit sky, their eyes met across the garden and \"\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "Under the moonlit sky, their eyes met across the garden and all the\n",
      "day was by a string was the stranger with her face and sake, and ask an extracement, and having his account of the subject\n",
      "was strong sentiment.\n",
      "\n",
      "Mrs.\n",
      "Jennings was the sake of her sister. But to\n",
      "be a first\n",
      "thoughts he should have been at any occasions to the constancy to any other as in the way without anything that she had seen the liveliest and self-confidence, without\n",
      "highly so handsome, he saw her sister and the falling her favourite\n",
      "close in a secrets were a comfort.\n",
      "\n",
      "“What a little\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt 3: \"The duchess had sworn never to love again, but \"\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "The duchess had sworn never to love again, but she was not all the\n",
      "same thanks, and the last stooping to be a little and strange family and this day.\n",
      "\n",
      "Miss Elizabeth, in an interesting the lovely and still obstacles, the satisfaction of the situation of the first answer, and therefore the case and still treating the way of the whole plain secrecy trite to accasion. Mr. Bingley and, was along any time after the window, to her high in the same account of her\n",
      "consideration of the\n",
      "same and the latter to accept his feelings with Mr. Wickham, and \n",
      "------------------------------------------------------------\n",
      "\n",
      "===== GENERATING TEXT FROM SCIFI_ROMANCE_HYBRID MODEL =====\n",
      "Using checkpoint: SciFi_Romance_Hybrid_final.pt\n",
      "\n",
      "Prompt 1: \"The space station's observation deck was where she first saw him, \"\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "The space station's observation deck was where she first saw him, or happen\n",
      "on the morning of his own house, had had not only of one of her\n",
      "own share in her own father at this house; but in the house had soon followed her\n",
      "sister’s hand, and the platform she had not seen him at length, and she was opened her\n",
      "father, she was at Longstaple, and saw them of every officers. But she could not\n",
      "     from the housekeeper of an object for a sense of shells. But she\n",
      "was enjoying them, that he had been expected to see her as they all heard, as she\n",
      "could not ever be sure h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt 2: \"The android felt something strange in its circuits whenever she smiled, \"\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "The android felt something strange in its circuits whenever she smiled, was\n",
      "so surmounted by her own face on her mother and Marianne, who was\n",
      "always by no means on the others, and though she was over her own sense. Her\n",
      "head, his subject was expected on her sister, she could not see her to have been in\n",
      "her own friend, was not a servant, and her own harpoon was not only one\n",
      "of her family was obliged to see him to speak of her sister, and she was not superior to her sister\n",
      "and the evening to Mrs. Palmer with a presence of his soul in the same time.\n",
      "\n",
      "CHAPTER II\n",
      "\n",
      "[_Copyr\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt 3: \"Time travel was forbidden, but he couldn't bear to live without her, \"\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "Time travel was forbidden, but he couldn't bear to live without her, who\n",
      "was an anxiety to say, and saw him so much so standing upon the\n",
      "possibility of speaking, and the same three shot has been in her sister’s\n",
      "pleasure to the evening of the same sense of his eyes. Besides, after all the others\n",
      "were singing a monster of her sister’s sea-cases.\n",
      "\n",
      "“No, sir, the sea of the Captain’s companions were to see it to the same thing\n",
      "that we are near the Captain, who had been so little and fastening a possession. The\n",
      "sun is not so far after the 21th of New River, and 10026 y\n",
      "------------------------------------------------------------\n",
      "\n",
      "===== CROSS-GENRE EXPERIMENT =====\n",
      "\n",
      "===== GENERATING TEXT FROM CLASSIC_ROMANCE_MODEL MODEL =====\n",
      "Using checkpoint: Classic_Romance_Model_final.pt\n",
      "\n",
      "Prompt 1: \"The starship's engines hummed as it approached the nebula, \"\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "The starship's engines hummed as it approached the nebula, and still the family seemed at him as well with the window with an\n",
      "acquaintance. The first she had been immediately descending the\n",
      "dress of any thing and all the same time in him, and was almost as to all as a\n",
      "subject of her account, and a concerning their frequent case who the candle as the same time as if they were always then both with the supper at first in the same idea of the colour which had\n",
      "been a fortnight in his case and desert a liberty, his single affection was all the family as a co\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt 2: \"In the year 2150, the first quantum AI achieved consciousness and \"\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "In the year 2150, the first quantum AI achieved consciousness and serious attention. I was as\n",
      "an opporting the window was so much of its\n",
      "rest of a stranger and advantage from herself.\n",
      "\n",
      "“I am not sent for a letter to take an intelligable attention.”\n",
      "\n",
      "“Is you are all a living to take the staircase.”\n",
      "\n",
      "“Then, is a mistake?”\n",
      "\n",
      "“You are at the constant satisfactions, and with you the\n",
      "whole,” said Mrs. Jennings, “is the\n",
      "wishes, and the strength in some of the\n",
      "world in my bright, she\n",
      "had so much of the same to ask me to another source, and had been said, she\n",
      "was a pain\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt 3: \"The alien artifact glowed with an eerie blue light when \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "The alien artifact glowed with an eerie blue light when something the same situation of a liberty. He had never came by a secrice was broken as the street. This hours\n",
      "had seen him to be a four accept in his sister’s.\n",
      "\n",
      "Marianne was such independence\n",
      "was an account of the foot of the favourably consequence of the last and talk of this disappointment who\n",
      "was strongly still; the clear\n",
      "attachment a commence of inclination on the same stations of her desire of his face of a survisit imagination and family,\n",
      "and so streamed the wishes of\n",
      "the way back and sai\n",
      "------------------------------------------------------------\n",
      "\n",
      "===== GENERATING TEXT FROM SCIFI_ADVENTURE_MODEL MODEL =====\n",
      "Using checkpoint: SciFi_Adventure_Model_final.pt\n",
      "\n",
      "Prompt 1: \"Her heart raced as she opened the letter, never expecting \"\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "Her heart raced as she opened the letter, never expecting the\n",
      "part, and the strangeness of a strange steamboat\n",
      "of the steward. I had been rested behind him. Hussians\n",
      "as it was the place, which astonished into the sea, and an instant to the\n",
      "rather state of sea who was, and when the steamer was still being at\n",
      "one time in the sea, the most propertures of that whole of me seen in the workshit\n",
      "of the streets of the straight or face, and I stood at him in a sight of\n",
      "sheet out of my mind.\n",
      "\n",
      "“What’s will be that I will be above it in any course. I was being tra\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt 2: \"Under the moonlit sky, their eyes met across the garden and \"\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "Under the moonlit sky, their eyes met across the garden and things which\n",
      "certainly a cold strange excursion of the sea.\n",
      "\n",
      "I hesitated at the correct was the bottom of the beautiful space of\n",
      "shipping in the matter of a sea of the panels and streaming and served the\n",
      "platform.\n",
      "\n",
      "“It is a moment the sun,” said the Captain, “that the _Nautilus_ has been\n",
      "resisted. The _Nautilus_ had seen the strangenery for imagined that we were the\n",
      "police of the seas, at the bearings of the moon to the\n",
      "compartment with a companion above them; and to shelter with the midst of\n",
      "th\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt 3: \"The duchess had sworn never to love again, but \"\n",
      "Generated text:\n",
      "------------------------------------------------------------\n",
      "The duchess had sworn never to love again, but I\n",
      "saw another in the rest of the surface of the surface\n",
      "of the _Nautilus_ seemed as a human repressively as if the resporting powerful\n",
      "excitement of the _Nautilus_.\n",
      "\n",
      "At this submarine heavy shaped struck and seemed to be seen, and\n",
      "the bottle succeeded the waves were as well as it stream. I could show my\n",
      "eyes were a fact, and that we had not already asked.\n",
      "\n",
      "“We are not the shark of a heap of sea-weed.”\n",
      "\n",
      "“Yes,” said Captain Nemo, in the meast was a means of privated half-past\n",
      "to be so soon as I ha\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate text from each model\n",
    "sci_fi_samples = generate_samples('SciFi_Adventure_Model', sci_fi_prompts, temperature=0.7, topk=5)\n",
    "romance_samples = generate_samples('Classic_Romance_Model', romance_prompts, temperature=0.7, topk=5)\n",
    "mixed_samples = generate_samples('SciFi_Romance_Hybrid', mixed_prompts, temperature=0.7, topk=5)\n",
    "\n",
    "# Cross-genre experiment: use sci-fi prompts with romance model and vice versa\n",
    "print(\"\\n===== CROSS-GENRE EXPERIMENT =====\")\n",
    "sci_fi_prompts_in_romance = generate_samples('Classic_Romance_Model', sci_fi_prompts, temperature=0.8)\n",
    "romance_prompts_in_scifi = generate_samples('SciFi_Adventure_Model', romance_prompts, temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SciFi_Model Statistics =====\n",
      "   Avg. Words  Avg. Sentences  Avg. Words/Sentence  Avg. Vocabulary Richness  \\\n",
      "0      108.67             5.0                23.31                      0.61   \n",
      "\n",
      "   Avg. Sci-Fi Keywords  Avg. Romance Keywords  \n",
      "0                   1.0                    0.0  \n",
      "\n",
      "===== Romance_Model Statistics =====\n",
      "   Avg. Words  Avg. Sentences  Avg. Words/Sentence  Avg. Vocabulary Richness  \\\n",
      "0      106.33            5.67                19.08                      0.68   \n",
      "\n",
      "   Avg. Sci-Fi Keywords  Avg. Romance Keywords  \n",
      "0                   0.0                    1.0  \n",
      "\n",
      "===== Mixed_Model Statistics =====\n",
      "   Avg. Words  Avg. Sentences  Avg. Words/Sentence  Avg. Vocabulary Richness  \\\n",
      "0       112.0            3.67                31.14                      0.63   \n",
      "\n",
      "   Avg. Sci-Fi Keywords  Avg. Romance Keywords  \n",
      "0                  0.33                    0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zmc/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "def analyze_text(text, genre_keywords=None):\n",
    "    \"\"\"Analyze text characteristics\"\"\"\n",
    "    # Basic statistics\n",
    "    word_count = len(text.split())\n",
    "    sentence_count = len([s for s in text.split('.') if s.strip()])\n",
    "    avg_words_per_sentence = word_count / max(1, sentence_count)\n",
    "    unique_words = len(set(text.lower().split()))\n",
    "    vocabulary_richness = unique_words / max(1, word_count)\n",
    "    \n",
    "    # Keyword matching\n",
    "    keyword_counts = {}\n",
    "    if genre_keywords:\n",
    "        for category, words in genre_keywords.items():\n",
    "            matches = sum(1 for word in words if word.lower() in text.lower())\n",
    "            keyword_counts[category] = matches\n",
    "    \n",
    "    return {\n",
    "        \"word_count\": word_count,\n",
    "        \"sentence_count\": sentence_count,\n",
    "        \"avg_words_per_sentence\": avg_words_per_sentence,\n",
    "        \"unique_words\": unique_words,\n",
    "        \"vocabulary_richness\": vocabulary_richness,\n",
    "        \"keyword_matches\": keyword_counts\n",
    "    }\n",
    "\n",
    "# Define genre-specific keywords\n",
    "keywords = {\n",
    "    \"sci_fi\": [\"space\", \"star\", \"galaxy\", \"robot\", \"alien\", \"future\", \"technology\", \n",
    "               \"quantum\", \"spacecraft\", \"planet\", \"artificial\", \"hologram\"],\n",
    "    \"romance\": [\"love\", \"heart\", \"passion\", \"kiss\", \"embrace\", \"emotion\", \"desire\",\n",
    "                \"longing\", \"romance\", \"affection\", \"intimate\", \"relationship\"]\n",
    "}\n",
    "\n",
    "# Analyze samples from each model\n",
    "all_results = {\n",
    "    \"SciFi_Model\": [analyze_text(sample, keywords) for sample in sci_fi_samples],\n",
    "    \"Romance_Model\": [analyze_text(sample, keywords) for sample in romance_samples],\n",
    "    \"Mixed_Model\": [analyze_text(sample, keywords) for sample in mixed_samples]\n",
    "}\n",
    "\n",
    "# Print summary statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "for model, results in all_results.items():\n",
    "    print(f\"\\n===== {model} Statistics =====\")\n",
    "    avg_results = {\n",
    "        \"Avg. Words\": np.mean([r[\"word_count\"] for r in results]),\n",
    "        \"Avg. Sentences\": np.mean([r[\"sentence_count\"] for r in results]),\n",
    "        \"Avg. Words/Sentence\": np.mean([r[\"avg_words_per_sentence\"] for r in results]),\n",
    "        \"Avg. Vocabulary Richness\": np.mean([r[\"vocabulary_richness\"] for r in results]),\n",
    "        \"Avg. Sci-Fi Keywords\": np.mean([r[\"keyword_matches\"][\"sci_fi\"] for r in results]),\n",
    "        \"Avg. Romance Keywords\": np.mean([r[\"keyword_matches\"][\"romance\"] for r in results])\n",
    "    }\n",
    "    df = pd.DataFrame([avg_results])\n",
    "    print(df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TEMPERATURE EXPERIMENT: SciFi_Adventure_Model =====\n",
      "Prompt: \"The alien signal was decoded and revealed \"\n",
      "\n",
      "Temperature: 0.5\n",
      "------------------------------------------------------------\n",
      "The alien signal was decoded and revealed in the\n",
      "last day that are were cleared on a shape of the seas, and the\n",
      "white forest of the natives had been surrounded into the shells of an astrains of the\n",
      "streets of the steamer.\n",
      "\n",
      "“The _Nautilus_ will allow the thing of this story as it is a pressure\n",
      "of the interior.”\n",
      "\n",
      "“No deserted its seas, and it\n",
      "------------------------------------------------------------\n",
      "\n",
      "Temperature: 0.8\n",
      "------------------------------------------------------------\n",
      "The alien signal was decoded and revealed on the\n",
      "arches.\n",
      "\n",
      "At my mind was never, upon us in a water of all my strong-mass.\n",
      "\n",
      "I thought that the waves getting one in the ottan sharp stilling\n",
      "with the day, according to\n",
      "earth it wishing, strength-headlong strace and sir, and the _Nautilus_ was accountanged\n",
      "into the shark in the words of Black ha\n",
      "------------------------------------------------------------\n",
      "\n",
      "Temperature: 1.1\n",
      "------------------------------------------------------------\n",
      "The alien signal was decoded and revealed that the atmosphere\n",
      "of a streak frame into plain, of with\n",
      "seaweed; and incled like way, if I did not mink rises up to the surface. And my\n",
      "breaking on board, dressed after toss the sea.\n",
      "\n",
      "If Captain Nemo struck passy soon sea-waters and real\n",
      "peacefuls, which had lattered the Gatter-Oitton, those\n",
      "cleve\n",
      "------------------------------------------------------------\n",
      "\n",
      "Temperature: 1.3\n",
      "------------------------------------------------------------\n",
      "The alien signal was decoded and revealed on ice\n",
      "and stopped with her oyphant; alundauses which a kitchen and its\n",
      "houselessly, and for with\n",
      "creature’s knuckless kibrea-thits one. This dividored\n",
      "nearly apparions had crouched us. But I had the jaw,\n",
      "I\n",
      "frumbled these flave. I Farthern eatingly, and more Roy took With Captain Nemo. I scragced he\n",
      "------------------------------------------------------------\n",
      "\n",
      "===== TEMPERATURE EXPERIMENT: Classic_Romance_Model =====\n",
      "Prompt: \"She found his old letters in the attic, \"\n",
      "\n",
      "Temperature: 0.5\n",
      "------------------------------------------------------------\n",
      "She found his old letters in the attic, the constant income in the same time and such an absolute parting a few months of distance in the same to Mr. Darcy, is the good and days acknowledged and said with a strange of the end of the mistake. The Miss Bingley was a book to his late at all that to her\n",
      "manners were blind in the partiality of\n",
      "------------------------------------------------------------\n",
      "\n",
      "Temperature: 0.8\n",
      "------------------------------------------------------------\n",
      "She found his old letters in the attic, that ease is a\n",
      "letter that I am\n",
      "interided yourself as well for me.”\n",
      "\n",
      "“I have been inexpected to wait the lamb of fortune if have been enough for him.”\n",
      "\n",
      "The cause of\n",
      "the last years again\n",
      "her affection, if he was received in a cold was so gassing a\n",
      "fairy assertions, and her; her probably desire of eye\n",
      "------------------------------------------------------------\n",
      "\n",
      "Temperature: 1.1\n",
      "------------------------------------------------------------\n",
      "She found his old letters in the attic, and Adèle and she will last\n",
      "holls outside Oh, if you do not you forgive him I have learnt, almost is by a notions. He\n",
      "require their enjoyment, cave in explanation, I\n",
      "am three young is\n",
      "small, I was\n",
      "sitting. My judgments and tears you\n",
      "may be more without high-minding?”\n",
      "\n",
      "“Yes, sir.” And captiva\n",
      "spirits\n",
      "------------------------------------------------------------\n",
      "\n",
      "Temperature: 1.3\n",
      "------------------------------------------------------------\n",
      "She found his old letters in the attic, and turned hillsed me home,—I\n",
      "have five\n",
      "to which\n",
      "Georgiana, I glazk like eacubsle, hair already convinced your goddlik, burst\n",
      "right.”\n",
      "\n",
      "Miss Ingram\n",
      "might pale. Puil reflectionate and smile\n",
      "Eliza.”\n",
      "\n",
      "“Of\n",
      "fits will have\n",
      "been really object to equal voop, anjoteen before Lozining alone, fecking\n",
      "carplanch \n",
      "------------------------------------------------------------\n",
      "\n",
      "===== TEMPERATURE EXPERIMENT: SciFi_Romance_Hybrid =====\n",
      "Prompt: \"The android was programmed to protect her, but it felt something more, \"\n",
      "\n",
      "Temperature: 0.5\n",
      "------------------------------------------------------------\n",
      "The android was programmed to protect her, but it felt something more, and she\n",
      "stood on a strong one of a man had been so long as her sisters, and the\n",
      "Editor should not be sheet on the sincere seas of the evening of the house.\n",
      "\n",
      "[Illustration]\n",
      "\n",
      "[Illustration]\n",
      "\n",
      "“I do not preserve myself on the other place, that I am sure, and says that I am\n",
      "sure I have not the object of \n",
      "------------------------------------------------------------\n",
      "\n",
      "Temperature: 0.8\n",
      "------------------------------------------------------------\n",
      "The android was programmed to protect her, but it felt something more, and\n",
      "him round Wickham, who knew what to go to the hands of person, and\n",
      "less invitation from four hopes of shadows by the six valley of the like dim of\n",
      "North Park, the Medical Man on the Doctor. I asked it, against the evenings\n",
      "and depths of the burning trace of sounds over my intense acquaintance. T\n",
      "------------------------------------------------------------\n",
      "\n",
      "Temperature: 1.1\n",
      "------------------------------------------------------------\n",
      "The android was programmed to protect her, but it felt something more, Maria\n",
      "Marianne were dissead. He would do Marianne. _Then_ for\n",
      "since of ceremonious excessions grew overcame him for me.\n",
      "\n",
      "[TeRds and Miss Steele was serious into the Editors.\n",
      "\n",
      "“I see myself and here! You’ve been the same time I may indeed,” I\n",
      "sagned.\n",
      "\n",
      "Elinor, who jeeled his story, the information of \n",
      "------------------------------------------------------------\n",
      "\n",
      "Temperature: 1.3\n",
      "------------------------------------------------------------\n",
      "The android was programmed to protect her, but it felt something more, had\n",
      "to make jeins passing only ebon a favours, and\n",
      "painful pressitude. _You_ it don’t to Leater only from herself.”\n",
      "\n",
      "[Illustration]\n",
      "\n",
      "It answered Conseil, 1867.\n",
      "\n",
      "When Miss Darcyis he might tree in his mother receiving Charlotte’s vague packheom Louisa,\n",
      "that her mind had frequently last few of her the\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def temperature_experiment(model_name, prompt, temperatures=[0.5, 0.7, 0.9, 1.1]):\n",
    "    \"\"\"Generate text with different temperature settings\"\"\"\n",
    "    checkpoint_files = glob.glob(f'checkpoints_{model_name}/*.pt')\n",
    "    if not checkpoint_files:\n",
    "        return\n",
    "    \n",
    "    best_checkpoint = min(checkpoint_files, \n",
    "                         key=lambda x: float(x.split('val')[-1].split('.pt')[0]) \n",
    "                         if 'val' in x else float('inf'))\n",
    "    \n",
    "    print(f\"\\n===== TEMPERATURE EXPERIMENT: {model_name} =====\")\n",
    "    print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\nTemperature: {temp}\")\n",
    "        generated = sample_text(\n",
    "            model_path=best_checkpoint,\n",
    "            prime_text=prompt,\n",
    "            length=300,\n",
    "            temperature=temp,\n",
    "            topk=0\n",
    "        )\n",
    "        print(f\"{'-'*60}\\n{generated}\\n{'-'*60}\")\n",
    "\n",
    "# Run temperature experiments on each model\n",
    "temperature_experiment('SciFi_Adventure_Model', \"The alien signal was decoded and revealed \", \n",
    "                      temperatures=[0.5, 0.8, 1.1, 1.3])\n",
    "temperature_experiment('Classic_Romance_Model', \"She found his old letters in the attic, \",\n",
    "                      temperatures=[0.5, 0.8, 1.1, 1.3])\n",
    "temperature_experiment('SciFi_Romance_Hybrid', \"The android was programmed to protect her, but it felt something more, \",\n",
    "                      temperatures=[0.5, 0.8, 1.1, 1.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: \"The interstellar romance began when her spacecraft malfunctioned near his planet, \"\n",
      "Genre balance: 60.0% sci-fi / 40.0% romance\n",
      "Mixed sentences: 1/2 (50.0%)\n",
      "Text sample:\n",
      "------------------------------------------------------------\n",
      "The interstellar romance began when her spacecraft malfunctioned near his planet, and\n",
      "having roused his house and surprise to her the opposition of hope, and had offended\n",
      "it to him, and said,--\n",
      "\n",
      "“I had not the complacency of some of the Canadian’s proper sensation, who is a\n",
      "moment and strong surface...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt: \"His programming didn't include falling in love, but when she repaired his circuits, \"\n",
      "Genre balance: 50.0% sci-fi / 50.0% romance\n",
      "Mixed sentences: 0/4 (0.0%)\n",
      "Text sample:\n",
      "------------------------------------------------------------\n",
      "His programming didn't include falling in love, but when she repaired his circuits, or on the\n",
      "sun had been silent. And so the first part of the hull of the surface of these\n",
      "parts of the hills and front of an agreeable star was so far at the door of\n",
      "a second consequence of a content of the faintest t...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt: \"Traveling through time had consequences, but she was worth every paradox, \"\n",
      "Genre balance: N/A\n",
      "Mixed sentences: 0/6 (0.0%)\n",
      "Text sample:\n",
      "------------------------------------------------------------\n",
      "Traveling through time had consequences, but she was worth every paradox, and\n",
      "she had spared him; such an uniform expected to her own head.\n",
      "\n",
      "[Illustration]\n",
      "\n",
      "“I shall say to my customs, that why are the faint-tables,” said Ned Land, though\n",
      "the Canadian, “that these tenderness to the curiosity of the ...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def analyze_genre_fusion(text):\n",
    "    \"\"\"Analyze the fusion of genres in generated text\"\"\"\n",
    "    # Count genre-specific elements\n",
    "    sci_fi_elements = sum(1 for word in keywords[\"sci_fi\"] if word.lower() in text.lower())\n",
    "    romance_elements = sum(1 for word in keywords[\"romance\"] if word.lower() in text.lower())\n",
    "    \n",
    "    # Analyze sentence structure\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    mixed_sentences = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        has_scifi = any(word in sentence.lower() for word in keywords[\"sci_fi\"])\n",
    "        has_romance = any(word in sentence.lower() for word in keywords[\"romance\"])\n",
    "        if has_scifi and has_romance:\n",
    "            mixed_sentences += 1\n",
    "    \n",
    "    return {\n",
    "        \"sci_fi_elements\": sci_fi_elements,\n",
    "        \"romance_elements\": romance_elements,\n",
    "        \"total_sentences\": len(sentences),\n",
    "        \"mixed_sentences\": mixed_sentences,\n",
    "        \"genre_balance\": f\"{sci_fi_elements/(sci_fi_elements+romance_elements)*100:.1f}% sci-fi / {romance_elements/(sci_fi_elements+romance_elements)*100:.1f}% romance\" if (sci_fi_elements+romance_elements) > 0 else \"N/A\"\n",
    "    }\n",
    "\n",
    "# Generate fusion texts with mixed model\n",
    "fusion_prompts = [\n",
    "    \"The interstellar romance began when her spacecraft malfunctioned near his planet, \",\n",
    "    \"His programming didn't include falling in love, but when she repaired his circuits, \",\n",
    "    \"Traveling through time had consequences, but she was worth every paradox, \"\n",
    "]\n",
    "\n",
    "fusion_results = []\n",
    "for prompt in fusion_prompts:\n",
    "    generated = sample_text(\n",
    "        model_path=glob.glob('checkpoints_SciFi_Romance_Hybrid/*.pt')[0],\n",
    "        prime_text=prompt,\n",
    "        length=500,\n",
    "        temperature=0.8,\n",
    "        topk=5\n",
    "    )\n",
    "    \n",
    "    analysis = analyze_genre_fusion(generated)\n",
    "    fusion_results.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"text\": generated,\n",
    "        \"analysis\": analysis\n",
    "    })\n",
    "\n",
    "# Print fusion analysis\n",
    "for result in fusion_results:\n",
    "    print(f\"\\nPrompt: \\\"{result['prompt']}\\\"\")\n",
    "    print(f\"Genre balance: {result['analysis']['genre_balance']}\")\n",
    "    print(f\"Mixed sentences: {result['analysis']['mixed_sentences']}/{result['analysis']['total_sentences']} ({result['analysis']['mixed_sentences']/result['analysis']['total_sentences']*100:.1f}%)\")\n",
    "    print(f\"Text sample:\\n{'-'*60}\\n{result['text'][:300]}...\\n{'-'*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
